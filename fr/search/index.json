[{"content":"Depuis la version 3.2 d\u0026rsquo;NSX-T, un cluster Kubernetes est nécessaire afin d\u0026rsquo;installer NSX Application Platform qui permet ensuite d\u0026rsquo;installer NSX inteligence et d\u0026rsquo;autres composants de sécurités.\nDans cet article nous allons voir comment installer NSX Application Platform dans un environnement Kubernetes déployé par Container service Extension.\nNSX Application Platform avec CSE Pré requis Environnement NSX fonctionnel Environnement CSE fonctionnel Quelques connaissances Kubernetes Déploiement Cluster Kubernetes Nous allons déployer NSX Application Platform en mode évaluation, en accord avec la documentation VMware vous devez déployer :\nUn seul Master node\n2 vCPUs 4 GB RAM Un seul Work Node\n16 vCPUs 64 GB RAM Dans un premier temps, au sein de votre Cloud Director vous devez créer une nouvelle VM sizing policy\nVous pouvez ensuite déployer votre cluster Kubernetes avec Container Service Extension\nVous devez augmenter l\u0026rsquo;espace disque des worker node afin d\u0026rsquo;avoir suffisamment d\u0026rsquo;espace libre pour télécharger les images nécessaires à NSX Application platform\nConfiguration Cluster Kubernetes Persistent Volumes Le driver de stockage par défaut est limité à 15 PVC par worker node. Cependant, NSX Application platorm nécessite environ 17 ou 18 PVC par Worker node. Vous devez donc configurer un autre driver de stockage. Vous pouvez utiliser le driver CSI NFS, un de mes précédents articles décrit la procédure d\u0026rsquo;installation :\nSetup NFS driver on Kubernetes TKGm cluster provide by Container Service Extension.\nClenning Certains composants installés par défaut par Container Service Extension sont incompatibles avec un nouveau déploiement d\u0026rsquo;NSX Application plateforme, vous devez supprimer les composants suivant :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 k delete CustomResourceDefinition k delete CustomResourceDefinition certificaterequests.cert-manager.io k delete CustomResourceDefinition certificates.cert-manager.io k delete CustomResourceDefinition challenges.acme.cert-manager.io k delete CustomResourceDefinition clusterissuers.cert-manager.io k delete CustomResourceDefinition issuers.cert-manager.io k delete CustomResourceDefinition orders.acme.cert-manager.io k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-controller-issuers k delete clusterRole cert-manager-controller-clusterissuers k delete clusterRole cert-manager-controller-certificates k delete clusterRole cert-manager-controller-orders k delete clusterRole cert-manager-controller-challenges k delete clusterRole cert-manager-controller-ingress-shim k delete clusterRole cert-manager-view k delete clusterRole cert-manager-edit k delete clusterRole cert-manager-controller-approve:cert-manager-io k delete clusterRole cert-manager-controller-certificatesigningrequests k delete clusterRole cert-manager-webhook:subjectaccessreviews k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-cainjector k delete ClusterRoleBinding cert-manager-cainjector k delete ClusterRoleBinding cert-manager-controller-issuers k delete ClusterRoleBinding cert-manager-controller-clusterissuers k delete ClusterRoleBinding cert-manager-controller-certificates k delete ClusterRoleBinding cert-manager-controller-orders k delete ClusterRoleBinding cert-manager-controller-challenges k delete ClusterRoleBinding cert-manager-controller-ingress-shim k delete ClusterRoleBinding cert-manager-controller-approve:cert-manager-io k delete ClusterRoleBinding cert-manager-controller-certificatesigningrequests k delete ClusterRoleBinding cert-manager-webhook:subjectaccessreview k delete ClusterRoleBinding cert-manager-webhook:subjectaccessreviews k delete ClusterRoleBinding cert-manager-cainjector:leaderelection k delete ClusterRoleBinding cert-manager-cainjector k delete roles cert-manager:leaderelection -n kube-system k delete roles cert-manager-cainjector:leaderelection -n kube-system k delete ClusterRoleBinding cert-manager-cainjector:leaderelection -n kube-system k delete RoleBinding cert-manager-cainjector:leaderelection -n kube-system k delete RoleBinding cert-manager:leaderelection -n kube-system k delete MutatingWebhookConfiguration cert-manager-webhook -n kube-system k delete ValidatingWebhookConfiguration cert-manager-webhook -n kube-system k delete namespaces cert-manager Installation Vous pouvez maintenant lancer le Wizzard d\u0026rsquo;installation. N\u0026rsquo;oubliez pas de sélectionner la storage classe adéquate :\nLe FQDN Interface Service Name correspond à l\u0026rsquo;IP du service contour déployé par NSX Application platform. Durant le déploiement vous pouvez déterminer l\u0026rsquo;IP assigné au service Contour et modifier vos enregistrements DNS en fonction.\nSi vous avez correctement supprimé l\u0026rsquo;ensemble des ressources dépendantes de cert-manager, déployé le nombre correct de Master et Worker node, et respecté les ressources hardware indiquées dans la documentation VMware, l\u0026rsquo;ensemble des check devrait être réussi\nAprès quelques minutes vous serez en mesure d\u0026rsquo;activer NSX Intelligence et l\u0026rsquo;utiliser sur votre environnement\n","date":"2023-07-15T00:00:00Z","image":"https://www.loris-lombardi.cloud/fr/p/nsx-application-platform-cse/NSX_Application_Platform_hu64c2f9d239ba330faa306d5cc1b9bca5_126085_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.loris-lombardi.cloud/fr/p/nsx-application-platform-cse/","title":"Installer NSX Application platform avec CSE"},{"content":"Dans cet article nous allons configurer l\u0026rsquo;autoscaling d\u0026rsquo;un pool worker node pour un cluster Kubernetes déployé avec Container Service Extension 4.0.3. Cette fonctionnalité est très utile lorsque vous utilisez des HPA (Horizontal Pod Autoscaling). Elle permet de déployer des Worker node supplémentaires en fonction de la charge nécessaire pour vos applications.\nPré requis Un cluster Kubernetes fonctionnel déployé avec Container Service Extension Quelques connaissances Kubernetes Environnement de Test 1 2 3 4 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 21h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 21h v1.21.11+vmware.1 1 Master node : 2vCPU ; 4Go RAM 1 Worker node : 2vCPU ; 4Go RAM Configuration HPA Configuration Metrics Si cela n\u0026rsquo;est pas déjà fait vous devez déployer metrics-server sur votre cluster Kubernetes\n1 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Nous devons éditer le déploiement du metric server de la manière suivante\n1 k edit deployments.apps metrics-server -n kube-system Ajoutez l\u0026rsquo;option \u0026ndash;kubelet-insecure-tls\n1 2 3 4 5 6 7 8 9 10 containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=ExternalIP,Hostname,InternalIP - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls image: registry.k8s.io/metrics-server/metrics-server:latest imagePullPolicy: IfNotPresent Test HPA À partir du template suivant, nous allons configurer un déploiement et lui associer une politique HPA\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # hpa-test.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hpa-example spec: replicas: 1 selector: matchLabels: app: hpa-example template: metadata: labels: app: hpa-example spec: containers: - name: hpa-example image: gcr.io/google_containers/hpa-example ports: - name: http-port containerPort: 80 resources: requests: cpu: 200m --- apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: hpa-example-autoscaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-example minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 50 1 k apply -f hpa-test.yaml Vérification\n1 2 3 4 5 6 7 8 9 10 11 12 13 k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE hpa-example 1/1 1 1 3m21s k get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 0%/50% 1 10 1 28m k get pod NAME READY STATUS RESTARTS AGE hpa-example-cb54bb958-cggfp 1/1 Running 0 3m26s nginx-nfs-example 1/1 Running 0 12h Nous créons ensuite une VIP de type Load-balancer pour notre déploiement\n1 k expose deployment hpa-example --type=LoadBalancer --port=80 Dans notre exemple le load-balancer assigne l\u0026rsquo;IP 172.31.7.210\n1 2 3 4 k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hpa-example LoadBalancer 100.68.137.181 172.31.7.210 80:32258/TCP 14s kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 20h Monter en charge Nous allons maintenant faire monter en charge notre application\n1 2 3 4 5 6 7 8 9 10 # pod-wget.yaml apiVersion: v1 kind: Pod metadata: name: pod-wget spec: containers: - name: alpine image: alpine:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;infinity\u0026#39;] 1 2 3 4 k apply -f po-wget.yaml k exec -it pod-wget -- sh / # while true; do wget -q -O- http://172.31.7.210;done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! Après quelques minutes on peut voir que la fonctionnalité HPA entre en action\nSaturation de la charge CPU des pod\n1 2 3 k get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 381%/50% 1 10 4 35m On peut voir que l\u0026rsquo;HPA essaye de créer de nouveau POD cependant les ressources CPU du worker node sont également saturés\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k get pod NAME READY STATUS RESTARTS AGE hpa-example-cb54bb958-2sqpt 0/1 Pending 0 67s hpa-example-cb54bb958-44k4x 0/1 Pending 0 52s hpa-example-cb54bb958-6vd5l 0/1 Pending 0 52s hpa-example-cb54bb958-82fb4 0/1 Pending 0 82s hpa-example-cb54bb958-dpwwc 1/1 Running 0 40m hpa-example-cb54bb958-ltb96 0/1 Pending 0 67s hpa-example-cb54bb958-nsd54 0/1 Pending 0 67s hpa-example-cb54bb958-w74fx 0/1 Pending 0 82s hpa-example-cb54bb958-wz54z 1/1 Running 0 82s hpa-example-cb54bb958-zrgw4 0/1 Pending 0 67s k describe pod hpa-example-cb54bb958-2sqp Warning FailedScheduling 11s (x3 over 82s) default-scheduler 0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Activation de l\u0026rsquo;autoscaling Cette fonctionnalité n\u0026rsquo;est pas activée par défaut sur les cluster Kubernetes déployés par CSE et n\u0026rsquo;est pas encore implémentée par VMware. Voici une méthode pas à pas pour configurer cette fonctionnalité en attendant que celle-ci soit intégrée dans la road-map de développement.\nPréparation des composants nécessaires Identifier le namespace admin de votre cluster. Dans cet exemple mon cluster est nommé k8slorislombardi\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 k get namespaces NAME STATUS AGE capi-kubeadm-bootstrap-system Active 21h capi-kubeadm-control-plane-system Active 21h capi-system Active 21h capvcd-system Active 21h cert-manager Active 21h default Active 21h hpa-test Active 93m k8slorislombardi-ns Active 21h kube-node-lease Active 21h kube-public Active 21h kube-system Active 21h nfs-csi Active 15h rdeprojector-system Active 21h tanzu-package-repo-global Active 21h tkg-system Active 21h tkg-system-public Active 21h tkr-system Active 21h Le namespace \u0026ldquo;admin\u0026rdquo; de ce cluster est donc k8slorislombardi-ns\nToutes les étapes ci-dessous sont à réaliser dans le namespace \u0026ldquo;admin\u0026rdquo; : k8slorislombardi-ns\nCréer un pod temporaire et un PVC contenant notre kubeconfig 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #autoscale-config.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: named-disk.csi.cloud-director.vmware.com name: pvc-autoscaler spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Mi storageClassName: default-storage-class-1 volumeMode: Filesystem --- apiVersion: v1 kind: Pod metadata: name: pod-temporaire spec: containers: - name: alpine image: alpine:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;infinity\u0026#39;] volumeMounts: - name: pvc-autoscaler mountPath: /data volumes: - name: pvc-autoscaler persistentVolumeClaim: claimName: pvc-autoscaler Application et vérification\n1 2 3 4 5 6 7 8 9 k apply -f autoscale-config.yaml k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-autoscaler Bound pvc-775762d2-34e7-4854-823d-8f757d94437e 10Mi RWO default-storage-class-1 3m8s k get pod NAME READY STATUS RESTARTS AGE pod-temporaire 1/1 Running 0 2m57s Copie du fichier kubeconfig 1 2 3 4 5 k exec -it pod-temporaire -- sh / # / # cd /data /data # vi config Copiez le contenu de votre kubeconfig dans un nouveau fichier nommé config\nVous pouvez ensuite supprimer le pod-temporaire\n1 k delete pod pod-temporaire Configurer les ressources hardware du pool worker-node 1 2 3 4 5 k get machinedeployments.cluster.x-k8s.io NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION k8slorislombardi-worker-node-pool-1 k8slorislombardi 1 1 1 0 Running 23h v1.21.11+vmware.1 k edit machinedeployments.cluster.x-k8s.io k8slorislombardi-worker-node-pool-1 Nous modifions les paramètres de la propriété machinedeployments.cluster.x-k8s.io afin de définir :\nLe nombre maximum et minimum de Worker-node présent dans le pool Les ressources hardware des worker-node à déployer 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;MachineDeployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;creationTimestamp\u0026#34;:null,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;clusterName\u0026#34;:\u0026#34;k8slorislombardi\u0026#34;,\u0026#34;replicas\u0026#34;:1,\u0026#34;selector\u0026#34;:{},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;spec\u0026#34;:{\u0026#34;bootstrap\u0026#34;:{\u0026#34;configRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;bootstrap.cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;KubeadmConfigTemplate\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;}},\u0026#34;clusterName\u0026#34;:\u0026#34;k8slorislombardi\u0026#34;,\u0026#34;infrastructureRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;infrastructure.cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VCDMachineTemplate\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;},\u0026#34;version\u0026#34;:\u0026#34;v1.21.11+vmware.1\u0026#34;}}},\u0026#34;status\u0026#34;:{\u0026#34;availableReplicas\u0026#34;:0,\u0026#34;readyReplicas\u0026#34;:0,\u0026#34;replicas\u0026#34;:0,\u0026#34;unavailableReplicas\u0026#34;:0,\u0026#34;updatedReplicas\u0026#34;:0}} cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;5\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; capacity.cluster-autoscaler.kubernetes.io/memory: \u0026#34;4\u0026#34; capacity.cluster-autoscaler.kubernetes.io/cpu: \u0026#34;2\u0026#34; capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: \u0026#34;20Gi\u0026#34; capacity.cluster-autoscaler.kubernetes.io/maxPods: \u0026#34;200\u0026#34; machinedeployment.clusters.x-k8s.io/revision: \u0026#34;1\u0026#34; Déploiement de l\u0026rsquo;autoscaler Nous utilisons le yaml suivant pour qu’il utilise le PVC précédemment créé afin de récupérer le kubeconfig.\nVeillez à bien remplacer k8slorislombardi et k8slorislombardi-ns par les paramètres de votre cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: k8slorislombardi-ns labels: app: cluster-autoscaler spec: selector: matchLabels: app: cluster-autoscaler replicas: 1 template: metadata: labels: app: cluster-autoscaler spec: containers: - image: us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.20.0 name: cluster-autoscaler command: - /cluster-autoscaler args: - --cloud-provider=clusterapi - --kubeconfig=/data/config - --cloud-config=/data/config - --node-group-auto-discovery=clusterapi:clusterName=k8slorislombardi - --namespace=k8slorislombardi-ns - --node-group-auto-discovery=clusterapi:namespace=default volumeMounts: - name: pvc-autoscaler mountPath: /data volumes: - name: pvc-autoscaler persistentVolumeClaim: claimName: pvc-autoscaler serviceAccountName: cluster-autoscaler terminationGracePeriodSeconds: 10 --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-workload roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-workload subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: k8slorislombardi-ns --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-management roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-management subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: k8slorislombardi-ns --- apiVersion: v1 kind: ServiceAccount metadata: name: cluster-autoscaler namespace: k8slorislombardi-ns --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-workload rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces - persistentvolumeclaims - persistentvolumes - pods - replicationcontrollers - services verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - list - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/eviction verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - storage.k8s.io resources: - csinodes - storageclasses - csidrivers - csistoragecapacities verbs: - get - list - watch - apiGroups: - batch resources: - jobs verbs: - list - watch - apiGroups: - apps resources: - daemonsets - replicasets - statefulsets verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - delete - get - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - get - update --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-management rules: - apiGroups: - cluster.x-k8s.io resources: - machinedeployments - machinedeployments/scale - machines - machinesets verbs: - get - list - update - watch Vérification\n1 2 3 4 5 6 7 8 9 10 11 k apply -f .\\autoscale.yaml deployment.apps/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler-workload created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler-management created serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-workload created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-management created k get pod NAME READY STATUS RESTARTS AGE cluster-autoscaler-79c5cb9df6-gqlfd 1/1 Running 0 29s Configuration rdeprojector Par défaut Cloud director vérifie en permanence la configuration de votre custer Kubernetes via contrôleur rdeprojector. Ce contrôleur permet également d\u0026rsquo;ajouter ou supprimer des worker node à partir de l\u0026rsquo;interface graphique de cloud director\nLe contrôleur rdeprojector ne peut pas fonctionner avec la configuration autoscale que nous venons de déployer. En effet l\u0026rsquo;autoscaler va déployer un nouveau worker-node, rdeprojector va détecter une incohérence et donc supprimer le nouveau worker-node. A noter qu\u0026rsquo;il ne sera plus possible de modifier la configuration de votre cluster à partir de l\u0026rsquo;interface graphique de Cloud Director.\n1 k edit deployments.apps rdeprojector-controller-manager -n rdeprojector-system Modifier le nombre de replicas à 0\n1 2 3 4 5 6 7 spec: progressDeadlineSeconds: 600 replicas: 0 revisionHistoryLimit: 10 selector: matchLabels: control-plane: controller-manager Test Autoscaling 1 2 3 4 k apply -f po-wget.yaml k exec -it pod-wget -- sh / # while true; do wget -q -O- http://172.31.7.210;done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! Comme vu précédemment quelques minutes plus tard le CPU des Pod et du Worker node sont saturés\n1 2 3 k get hpa -n default NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 122%/50% 1 10 8 3h17m L\u0026rsquo;autoscaler entre en action et déploiement un nouveau Worker-node\n1 2 3 k get machinedeployments.cluster.x-k8s.io NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION k8slorislombardi-worker-node-pool-1 k8slorislombardi 2 1 2 1 ScalingUp 24h v1.21.11+vmware.1 Intégration automatique du nouveau Worker-node\n1 2 3 4 5 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 24h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-fk2xn NotReady \u0026lt;none\u0026gt; 3s v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 24h v1.21.11+vmware.1 Le nouveau Worker est actif\n1 2 3 4 5 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 24h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-fk2xn Ready \u0026lt;none\u0026gt; 42s v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 24h v1.21.11+vmware.1 Des pod supplémentaires sont déployés\n1 2 3 k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE hpa-example 10/10 10 10 3h29m Source : [cluster-autoscaler]https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\n","date":"2023-06-18T00:00:00Z","image":"https://www.loris-lombardi.cloud/fr/p/cse-kubernetes-hpa-autoscaling/cse-auto-scaling_hu6f5b223d0965fa925062afe10f8ce322_369195_120x120_fill_box_smart1_3.png","permalink":"https://www.loris-lombardi.cloud/fr/p/cse-kubernetes-hpa-autoscaling/","title":"Container Service Extension autoscaling"},{"content":"Dans cet article nous allons installer un nouveau driver de stockage pour les données persistantes de vos containers hébergés sur un cluster Tanzu TKGm fournis par Cloud Director et Container Service Extension (CSE).\nPourquoi utiliser un autre driver de stockage ? Les clusters déployés par CSE sont installés par défaut avec le driver cloud-director-named-disk-csi-driver\nCependant ce driver possède quelques limites, notamment sur les modes d\u0026rsquo;accès\nCSI Cloud Director Feature matrix\nFeature Support Scope Storage Type Independent Shareable Named Disks of VCD Provisioning Static Provisioning, Dynamic Provisioning Access Modes ReadOnlyMany , ReadWriteOnce Volume Block VolumeMode FileSystem Topology Static Provisioning: reuses VCD topology capabilities, Dynamic Provisioning: places disk in the OVDC of the ClusterAdminUser based on the StorageProfile specified. Comme nous pouvons le voir sur le tableau ci-dessus extrait de la documentation du driver par défaut, il n\u0026rsquo;est pas possible de créer des PVC de type ReadWriteMany. Cela peut être problématique dans le cas où plusieurs Pod souhaitent écrire de la data dans le même PVC.\nAvec ce driver il n\u0026rsquo;est pas non plus possible d\u0026rsquo;assigner plus de 15 volumes par worker node.\n1 2 3 4 5 6 7 8 9 10 11 // https://github.com/vmware/cloud-director-named-disk-csi-driver/blob/main/pkg/csi/node.go const ( // The maximum number of volumes that a node can have attached. // Since we\u0026#39;re using bus 1 only, it allows up-to 16 disks of which one (#7) // is pre-allocated for the HBA. Hence we have only 15 disks. maxVolumesPerNode = 15 DevDiskPath = \u0026#34;/dev/disk/by-path\u0026#34; ScsiHostPath = \u0026#34;/sys/class/scsi_host\u0026#34; HostNameRegexPattern = \u0026#34;^host[0-9]+\u0026#34; ) Installation du driver CSI NFS Pré requis Un cluster Kubernetes fonctionnel Quelques connaissances Kubernetes Helm Serveur NFS et PVC Dans cet exemple nous avons un cluster Kubernetes déployé par CSE 4.0.3 composé d\u0026rsquo;un master node et d\u0026rsquo;un worker node en version 1.21\n1 2 3 4 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 6h19m v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 6h16m v1.21.11+vmware.1 Nous installons le driver via helm\n1 2 helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system Nous devons modifier la configuration par défaut du chart Helm. Vous pouvez éditer directement la configuration des deployments et des daemonsets ou modifier le fichier values.yaml du repository helm\nModification dnsPolicy\n1 2 k edit deployments.apps csi-nfs-controller -n kube-system k edit daemonsets.apps csi-nfs-node -n kube-system Remplacer\n1 dnsPolicy: Default Par\n1 dnsPolicy: ClusterFirstWithHostNet Vérification\n1 2 3 4 k get pod -n kube-system | grep csi-nfs csi-nfs-controller-64cc5764b7-bd4p5 3/3 Running 0 6m37s csi-nfs-node-fp45z 3/3 Running 0 6m37s csi-nfs-node-vwf44 3/3 Running 0 6m37s Nous allons créer un PVC qui sera utilisé par notre serveur NFS. Ce PVC est créé via la driver par défaut : cloud-director-named-disk-csi-driver.\nVeillez à configurer la taille de ce PVC en fonction de vos besoins et de vos futurs besoins, car la taille de ce PVC ne pourra pas être modifiée\nDans cet exemple le driver cloud-director-named-disk-csi-driver est porté par la StorageClasse default-storage-class-1\n1 2 3 k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false Exemple d\u0026rsquo;un PVC pour le serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # pvc-nfs-server.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: named-disk.csi.cloud-director.vmware.com name: pvc-nfs-csi-server spec: accessModes: - ReadWriteOnce resources: requests: storage: 1024Gi storageClassName: default-storage-class-1 # Use your StorageClasse Name volumeMode: Filesystem Nous allons créer ensuite un serveur NFS dans un namespace dédié à cet usage. Ce template va créer un pod serveur NFS et créer également un service de type ClusterIP qui sera utilisé par une nouvelle StorageClass pour nos futurs PVC NFS\nExemple d\u0026rsquo;un serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 --- # nfs-server.yaml kind: Service apiVersion: v1 metadata: name: nfs-server labels: app: nfs-server spec: type: ClusterIP # use \u0026#34;LoadBalancer\u0026#34; to get a public ip selector: app: nfs-server ports: - name: tcp-2049 port: 2049 protocol: TCP - name: udp-111 port: 111 protocol: UDP --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-server spec: replicas: 1 selector: matchLabels: app: nfs-server template: metadata: name: nfs-server labels: app: nfs-server spec: nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux containers: - name: nfs-server image: itsthenetwork/nfs-server-alpine:latest env: - name: SHARED_DIRECTORY value: \u0026#34;/exports\u0026#34; volumeMounts: - mountPath: /exports name: pvc-nfs-csi-server securityContext: privileged: true ports: - name: tcp-2049 containerPort: 2049 protocol: TCP - name: udp-111 containerPort: 111 protocol: UDP volumes: - name: pvc-nfs-csi-server persistentVolumeClaim: claimName: pvc-nfs-csi-server Création du serveur NFS\n1 2 3 4 k create namespace nfs-csi kubens nfs-csi k apply -f pvc-nfs-server.yaml k apply -f nfs-server.yaml Vérification\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-csi-server Bound pvc-d61aa727-313d-4466-b923-6121a1ce93f7 1Ti RWO default-storage-class-1 3m17s k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nfs-server 1/1 1 1 76s k get pod NAME READY STATUS RESTARTS AGE nfs-server-56dfcc48c8-w759j 1/1 Running 0 2m24s k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nfs-server ClusterIP 100.64.116.170 \u0026lt;none\u0026gt; 2049/TCP,111/UDP 3m12s Étant donné que nous avons cré notre service dans le namespace nfs-csi celui-ci répond donc au fqdn nfs-server.nfs-csi.svc.cluster.local\nStorageClass NFS Pour finir, nous allons créer une nouvelle StorageClass qui utilisera le serveur NFS précédemment créé. Vous pouvez également utiliser un autre serveur NFS présent dans votre environnement (VM NFS, NFS NetApp\u0026hellip;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # StorageClass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi provisioner: nfs.csi.k8s.io parameters: server: nfs-server.nfs-csi.svc.cluster.local # FQDN or IP of NFS server share: / reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - nfsvers=4.1 allowVolumeExpansion: true 1 2 3 4 5 k apply -f StorageClass.yaml k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false 7h8m nfs-csi nfs.csi.k8s.io Delete Immediate true 18s Test du Driver NFS Création d\u0026rsquo;un PVC NFS et d\u0026rsquo;un Pod 1 2 3 4 5 6 7 8 9 10 11 12 #test-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: nfs-csi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- # nginx-nfs-example.yaml apiVersion: v1 kind: Pod metadata: name: nginx-nfs-example spec: containers: - image: nginx name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/www name: test-pvc volumes: - name: test-pvc persistentVolumeClaim: claimName: test-pvc 1 2 k apply -f test-pvc.yaml k apply -f nginx-nfs-example.yaml Vérification globale 1 2 3 4 5 6 7 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 5Gi RWX nfs-csi 7m41s k get pod NAME READY STATUS RESTARTS AGE nginx-nfs-example 1/1 Running 0 16s Serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubens nfs-csi k get pod NAME READY STATUS RESTARTS AGE nfs-server-7cccc9cc84-r2l7l 1/1 Running 0 65m k exec -it nfs-server-7cccc9cc84-r2l7l -- sh / # ls Dockerfile bin etc home media opt root sbin sys var README.md dev exports lib mnt proc run srv usr / # cd exports /exports # ls lost+found pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 /exports # Source : csi-driver-nfs\n","date":"2023-06-17T00:00:00Z","image":"https://www.loris-lombardi.cloud/fr/p/csi-nfs-container-service-extension/tkgm-csi-nfs_hu5ef57efeb7aec44e7c981e798331684a_122775_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.loris-lombardi.cloud/fr/p/csi-nfs-container-service-extension/","title":"TKGm Container Service Extension driver NFS"}]