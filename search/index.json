[{"content":"Dans cet article nous allons installer un nouveau driver de stockage pour les données persistantes de vos containers héberger sur un cluster Tanzu TKGm fournis par Cloud Director et Container Service Engine (CSE).\nPourquoi utiliser un autre driver de stockage ? Les clusters déployés par CSE sont installés par défaut avec le driver cloud-director-named-disk-csi-driver\nCependant ce driver possède quelques limites, notamment sur les modes d\u0026rsquo;accès\nCSI Cloud Director Feature matrix\nFeature Support Scope Storage Type Independent Shareable Named Disks of VCD Provisioning Static Provisioning, Dynamic Provisioning Access Modes ReadOnlyMany , ReadWriteOnce Volume Block VolumeMode FileSystem Topology Static Provisioning: reuses VCD topology capabilities, Dynamic Provisioning: places disk in the OVDC of the ClusterAdminUser based on the StorageProfile specified. Comme nous pouvons le voir sur le tableau ci-dessus extrait de la documentation du driver par défaut, il n\u0026rsquo;est pas possible de créer des PVC de type ReadWriteMany. Cela peut être problématique dans le cas où plusieurs Pod souhaitent écrire de la data dans le même PVC.\nAvec ce driver il n\u0026rsquo;est pas non plus possible d\u0026rsquo;assigner plus de 15 volumes par worker node.\n1 2 3 4 5 6 7 8 9 10 11 // https://github.com/vmware/cloud-director-named-disk-csi-driver/blob/main/pkg/csi/node.go const ( // The maximum number of volumes that a node can have attached. // Since we\u0026#39;re using bus 1 only, it allows up-to 16 disks of which one (#7) // is pre-allocated for the HBA. Hence we have only 15 disks. maxVolumesPerNode = 15 DevDiskPath = \u0026#34;/dev/disk/by-path\u0026#34; ScsiHostPath = \u0026#34;/sys/class/scsi_host\u0026#34; HostNameRegexPattern = \u0026#34;^host[0-9]+\u0026#34; ) Installation du driver CSI NFS Pré requis Un cluster Kubernetes fonctionnel Quelques connaissances Kubernetes Helm Serveur NFS et PVC Dans cet exemple nous avons un cluster Kubernetes déployé par CSE 4.0.3 composée d\u0026rsquo;un master node et d\u0026rsquo;un worker node en version 1.21\n1 2 3 4 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 6h19m v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 6h16m v1.21.11+vmware.1 Nous installons le driver via helm\n1 2 helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system Nous devons modifier la configuration par défaut du chart Helm. Vous pouvez éditer directement la configuration des deployments et des daemonsets ou modifier le fichier values.yaml du repository helm\nModification dnsPolicy\n1 2 k edit deployments.apps csi-nfs-controller -n kube-system k edit daemonsets.apps csi-nfs-node -n kube-system Remplacer\n1 dnsPolicy: Default Par\n1 dnsPolicy: ClusterFirstWithHostNet Vérification\n1 2 3 4 k get pod -n kube-system | grep csi-nfs csi-nfs-controller-64cc5764b7-bd4p5 3/3 Running 0 6m37s csi-nfs-node-fp45z 3/3 Running 0 6m37s csi-nfs-node-vwf44 3/3 Running 0 6m37s Nous allons créer un PVC qui sera utilisé par notre serveur NFS. Ce PVC est créé via la driver par défaut : cloud-director-named-disk-csi-driver.\nVeillez à configurer la taille de ce PVC en fonction de vos besoins et de vos futurs besoins, car la taille de ce PVC ne pourra pas être modifiée\nDans cet exemple le driver cloud-director-named-disk-csi-driver est porté par la StorageClasse default-storage-class-1\n1 2 3 k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false Exemple d\u0026rsquo;un PVC pour le serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # pvc-nfs-server.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: named-disk.csi.cloud-director.vmware.com name: pvc-nfs-csi-server spec: accessModes: - ReadWriteOnce resources: requests: storage: 1024Gi storageClassName: default-storage-class-1 # Use your StorageClasse Name volumeMode: Filesystem Nous allons créer ensuite un serveur NFS dans un namespace dédié à cet usage. Ce template va créer un pod serveur NFS et créer également un service de type ClusterIP qui sera utilisé par une nouvelle StorageClass pour nos futurs PVC NFS\nExemple d\u0026rsquo;un serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 --- # nfs-server.yaml kind: Service apiVersion: v1 metadata: name: nfs-server labels: app: nfs-server spec: type: ClusterIP # use \u0026#34;LoadBalancer\u0026#34; to get a public ip selector: app: nfs-server ports: - name: tcp-2049 port: 2049 protocol: TCP - name: udp-111 port: 111 protocol: UDP --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-server spec: replicas: 1 selector: matchLabels: app: nfs-server template: metadata: name: nfs-server labels: app: nfs-server spec: nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux containers: - name: nfs-server image: itsthenetwork/nfs-server-alpine:latest env: - name: SHARED_DIRECTORY value: \u0026#34;/exports\u0026#34; volumeMounts: - mountPath: /exports name: pvc-nfs-csi-server securityContext: privileged: true ports: - name: tcp-2049 containerPort: 2049 protocol: TCP - name: udp-111 containerPort: 111 protocol: UDP volumes: - name: pvc-nfs-csi-server persistentVolumeClaim: claimName: pvc-nfs-csi-server Création du serveur NFS\n1 2 3 4 k create namespace nfs-csi kubens nfs-csi k apply -f pvc-nfs-server.yaml k apply -f nfs-server.yaml Vérification\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-csi-server Bound pvc-d61aa727-313d-4466-b923-6121a1ce93f7 1Ti RWO default-storage-class-1 3m17s k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nfs-server 1/1 1 1 76s k get pod NAME READY STATUS RESTARTS AGE nfs-server-56dfcc48c8-w759j 1/1 Running 0 2m24s k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nfs-server ClusterIP 100.64.116.170 \u0026lt;none\u0026gt; 2049/TCP,111/UDP 3m12s Étant donné que nous avons créer notre service dans le namespace nfs-csi celui-ci répond donc au fqdn nfs-server.nfs-csi.svc.cluster.local\nStorageClass NFS Pour finir, nous allons créer une nouvelle StorageClass qui utilisera le serveur NFS précédemment créer. Vous pouvez également utiliser un autre serveur NFS présent dans votre environnement (VM NFS, NFS NetApp\u0026hellip;)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # StorageClass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi provisioner: nfs.csi.k8s.io parameters: server: nfs-server.nfs-csi.svc.cluster.local # FQDN or IP of NFS server share: / reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - nfsvers=4.1 allowVolumeExpansion: true 1 2 3 4 5 k apply -f StorageClass.yaml k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false 7h8m nfs-csi nfs.csi.k8s.io Delete Immediate true 18s Test du Driver NFS Création d\u0026rsquo;un PVC NFS et d\u0026rsquo;un Pod 1 2 3 4 5 6 7 8 9 10 11 12 #test-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: nfs-csi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- # nginx-nfs-example.yaml apiVersion: v1 kind: Pod metadata: name: nginx-nfs-example spec: containers: - image: nginx name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/www name: test-pvc volumes: - name: test-pvc persistentVolumeClaim: claimName: test-pvc 1 2 k apply -f test-pvc.yaml k apply -f nginx-nfs-example.yaml Vérification globale 1 2 3 4 5 6 7 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 5Gi RWX nfs-csi 7m41s k get pod NAME READY STATUS RESTARTS AGE nginx-nfs-example 1/1 Running 0 16s Serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubens nfs-csi k get pod NAME READY STATUS RESTARTS AGE nfs-server-7cccc9cc84-r2l7l 1/1 Running 0 65m k exec -it nfs-server-7cccc9cc84-r2l7l -- sh / # ls Dockerfile bin etc home media opt root sbin sys var README.md dev exports lib mnt proc run srv usr / # cd exports /exports # ls lost+found pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 /exports # Source : csi-driver-nfs\n","date":"2023-06-17T00:00:00Z","image":"https://www.loris-lombardi.cloud/p/csi-nfs-container-service-engine/tkgm-csi-nfs_hu5ef57efeb7aec44e7c981e798331684a_122775_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.loris-lombardi.cloud/p/csi-nfs-container-service-engine/","title":"TKGm Container Service Engine driver NFS"}]