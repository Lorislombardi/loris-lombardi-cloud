[{"content":"Since version 3.2 of NSX-T, a Kubernetes cluster is required to install NSX Application Platform, which to use NSX inteligence and other security components to be installed.\nIn this article, we\u0026rsquo;ll look at how to install NSX Application Platform in a Kubernetes environment deployed by Container service engine.\nNSX Application Platform with CSE Requirements Functional NSX environment Functional CSE environment Some knowledge of Kubernetes Deploying the Kubernetes cluster We are going to deploy NSX Application Platform in evaluation mode, according to VMware documentation you must deploy :\nOnly one Master node\n2 vCPUs 4 GB RAM Only one Work Node\n16 vCPUs 64 GB RAM First, create a new VM sizing policy in your Cloud Director.\nAfter this, you can then deploy your Kubernetes cluster with Container Service Engine.\nYou need to increase the disk space of worker node in order to have enough free space to download the images required by NSX Application platform.\nKubernetes cluster configuration Persistent Volumes The default storage driver is limited to 15 PVC per worker node. However, NSX Application platorm requires around 17 or 18 PVCs per worker node. So you need to configure another storage driver. You can use the CSI NFS driver. One of my previous articles describes the installation procedure:\nSetup NFS driver on Kubernetes TKGm cluster provide by Container Service Engine.\nClenning Some components installed by default by Container Service Engine are incompatible with a new deployment of NSX Application Platform, you must remove the following components:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 k delete CustomResourceDefinition k delete CustomResourceDefinition certificaterequests.cert-manager.io k delete CustomResourceDefinition certificates.cert-manager.io k delete CustomResourceDefinition challenges.acme.cert-manager.io k delete CustomResourceDefinition clusterissuers.cert-manager.io k delete CustomResourceDefinition issuers.cert-manager.io k delete CustomResourceDefinition orders.acme.cert-manager.io k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-controller-issuers k delete clusterRole cert-manager-controller-clusterissuers k delete clusterRole cert-manager-controller-certificates k delete clusterRole cert-manager-controller-orders k delete clusterRole cert-manager-controller-challenges k delete clusterRole cert-manager-controller-ingress-shim k delete clusterRole cert-manager-view k delete clusterRole cert-manager-edit k delete clusterRole cert-manager-controller-approve:cert-manager-io k delete clusterRole cert-manager-controller-certificatesigningrequests k delete clusterRole cert-manager-webhook:subjectaccessreviews k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-cainjector k delete clusterRole cert-manager-cainjector k delete ClusterRoleBinding cert-manager-cainjector k delete ClusterRoleBinding cert-manager-controller-issuers k delete ClusterRoleBinding cert-manager-controller-clusterissuers k delete ClusterRoleBinding cert-manager-controller-certificates k delete ClusterRoleBinding cert-manager-controller-orders k delete ClusterRoleBinding cert-manager-controller-challenges k delete ClusterRoleBinding cert-manager-controller-ingress-shim k delete ClusterRoleBinding cert-manager-controller-approve:cert-manager-io k delete ClusterRoleBinding cert-manager-controller-certificatesigningrequests k delete ClusterRoleBinding cert-manager-webhook:subjectaccessreview k delete ClusterRoleBinding cert-manager-webhook:subjectaccessreviews k delete ClusterRoleBinding cert-manager-cainjector:leaderelection k delete ClusterRoleBinding cert-manager-cainjector k delete roles cert-manager:leaderelection -n kube-system k delete roles cert-manager-cainjector:leaderelection -n kube-system k delete ClusterRoleBinding cert-manager-cainjector:leaderelection -n kube-system k delete RoleBinding cert-manager-cainjector:leaderelection -n kube-system k delete RoleBinding cert-manager:leaderelection -n kube-system k delete MutatingWebhookConfiguration cert-manager-webhook -n kube-system k delete ValidatingWebhookConfiguration cert-manager-webhook -n kube-system k delete namespaces cert-manager Installation You can now launch the installation Wizzard. Don\u0026rsquo;t forget to select the appropriate storage class:\nThe FQDN Interface Service Name corresponds to the IP of the contour service deployed by the NSX Application platform. During deployment, you can determine the IP assigned to the Contour service and modify your DNS records.\nIf you have correctly removed all cert-manager dependency, deployed the correct number of Master and Worker nodes, and respected the hardware resources requirement acording in to the VMware documentation, the check should be successful.\nAfter a few minutes, you\u0026rsquo;ll be able to activate NSX Intelligence and use it on your environment.\n","date":"2023-07-15T00:00:00Z","image":"https://www.loris-lombardi.cloud/p/nsx-application-platform-cse/NSX_Application_Platform_hu64c2f9d239ba330faa306d5cc1b9bca5_126085_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.loris-lombardi.cloud/p/nsx-application-platform-cse/","title":"Setup NSX Application platform with CSE"},{"content":"In this article we\u0026rsquo;ll configure worker node autoscaling for a Kubernetes cluster deployed with Contaier Service Engine 4.0.3. This feature is very useful when using HPA (Horizontal Pod Autoscaling). It allows you to deploy additional Worker nodes according to the load required by your applications.\nRequirements A working Kubernetes cluster deployed with Container Service Engine Some knowledge of Kubernetes Test environment 1 2 3 4 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 21h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 21h v1.21.11+vmware.1 1 Master node : 2vCPU ; 4Go RAM 1 Worker node : 2vCPU ; 4Go RAM Configuration HPA Metrics configuration If you haven\u0026rsquo;t already done so, you need to deploy metrics-server on your Kubernetes cluster.\n1 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml We need to edit the metric server deployment as follows\n1 k edit deployments.apps metrics-server -n kube-system Enable option \u0026ndash;kubelet-insecure-tls\n1 2 3 4 5 6 7 8 9 10 containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=ExternalIP,Hostname,InternalIP - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls image: registry.k8s.io/metrics-server/metrics-server:latest imagePullPolicy: IfNotPresent HPA test From the following template, we will configure a deployment and associate it with an HPA policy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # hpa-test.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hpa-example spec: replicas: 1 selector: matchLabels: app: hpa-example template: metadata: labels: app: hpa-example spec: containers: - name: hpa-example image: gcr.io/google_containers/hpa-example ports: - name: http-port containerPort: 80 resources: requests: cpu: 200m --- apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: hpa-example-autoscaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-example minReplicas: 1 maxReplicas: 100 targetCPUUtilizationPercentage: 50 1 k apply -f hpa-test.yaml Check\n1 2 3 4 5 6 7 8 9 10 11 12 13 k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE hpa-example 1/1 1 1 3m21s k get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 0%/50% 1 10 1 28m k get pod NAME READY STATUS RESTARTS AGE hpa-example-cb54bb958-cggfp 1/1 Running 0 3m26s nginx-nfs-example 1/1 Running 0 12h We then create a Load-balancer VIP for our deployment\n1 k expose deployment hpa-example --type=LoadBalancer --port=80 In our example, the load-balancer assigns the IP 172.31.7.210\n1 2 3 4 k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hpa-example LoadBalancer 100.68.137.181 172.31.7.210 80:32258/TCP 14s kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 20h Load up We are now going to ramp up our application\n1 2 3 4 5 6 7 8 9 10 # pod-wget.yaml apiVersion: v1 kind: Pod metadata: name: pod-wget spec: containers: - name: alpine image: alpine:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;infinity\u0026#39;] 1 2 3 4 k apply -f po-wget.yaml k exec -it pod-wget -- sh / # while true; do wget -q -O- http://172.31.7.210;done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! After a few minutes, you can see the HPA feature in action\nHight CPU utilization for pod\n1 2 3 k get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 381%/50% 1 10 4 35m You can see that the HPA is trying to create a new POD, but the CPU resources of the worker node are also too hight\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k get pod NAME READY STATUS RESTARTS AGE hpa-example-cb54bb958-2sqpt 0/1 Pending 0 67s hpa-example-cb54bb958-44k4x 0/1 Pending 0 52s hpa-example-cb54bb958-6vd5l 0/1 Pending 0 52s hpa-example-cb54bb958-82fb4 0/1 Pending 0 82s hpa-example-cb54bb958-dpwwc 1/1 Running 0 40m hpa-example-cb54bb958-ltb96 0/1 Pending 0 67s hpa-example-cb54bb958-nsd54 0/1 Pending 0 67s hpa-example-cb54bb958-w74fx 0/1 Pending 0 82s hpa-example-cb54bb958-wz54z 1/1 Running 0 82s hpa-example-cb54bb958-zrgw4 0/1 Pending 0 67s k describe pod hpa-example-cb54bb958-2sqp Warning FailedScheduling 11s (x3 over 82s) default-scheduler 0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. Enable autoscaling This feature is not enabled by default on Kubernetes clusters deployed by CSE and is not yet implemented by VMware. Here\u0026rsquo;s a step-by-step method for configuring this feature while we wait for it to be integrated into the development roadmap.\nPreparing the necessary components Identify your cluster\u0026rsquo;s admin namespace. In this example, my cluster is named k8slorislombardi\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 k get namespaces NAME STATUS AGE capi-kubeadm-bootstrap-system Active 21h capi-kubeadm-control-plane-system Active 21h capi-system Active 21h capvcd-system Active 21h cert-manager Active 21h default Active 21h hpa-test Active 93m k8slorislombardi-ns Active 21h kube-node-lease Active 21h kube-public Active 21h kube-system Active 21h nfs-csi Active 15h rdeprojector-system Active 21h tanzu-package-repo-global Active 21h tkg-system Active 21h tkg-system-public Active 21h tkr-system Active 21h The \u0026ldquo;admin\u0026rdquo; namespace for this cluster is therefore k8slorislombardi-ns.\nAll the steps below must be performed in the \u0026ldquo;admin\u0026rdquo; namespace: k8slorislombardi-ns\nCreate a temporary pod and a PVC containing our kubeconfig 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #autoscale-config.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: named-disk.csi.cloud-director.vmware.com name: pvc-autoscaler spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Mi storageClassName: default-storage-class-1 volumeMode: Filesystem --- apiVersion: v1 kind: Pod metadata: name: pod-temporaire spec: containers: - name: alpine image: alpine:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;infinity\u0026#39;] volumeMounts: - name: pvc-autoscaler mountPath: /data volumes: - name: pvc-autoscaler persistentVolumeClaim: claimName: pvc-autoscaler Implementation and checking\n1 2 3 4 5 6 7 8 9 k apply -f autoscale-config.yaml k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-autoscaler Bound pvc-775762d2-34e7-4854-823d-8f757d94437e 10Mi RWO default-storage-class-1 3m8s k get pod NAME READY STATUS RESTARTS AGE pod-temporaire 1/1 Running 0 2m57s Copy kubeconfig file 1 2 3 4 5 k exec -it pod-temporaire -- sh / # / # cd /data /data # vi config Copy the contents of your kubeconfig into a new file named config\nYou can then delete the pod-temporary\n1 k delete pod pod-temporaire Configure worker-node pool hardware resources 1 2 3 4 5 k get machinedeployments.cluster.x-k8s.io NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION k8slorislombardi-worker-node-pool-1 k8slorislombardi 1 1 1 0 Running 23h v1.21.11+vmware.1 k edit machinedeployments.cluster.x-k8s.io k8slorislombardi-worker-node-pool-1 We modify the parameters of the machinedeployments.cluster.x-k8s.io property to define :\nThe maximum and minimum number of worker-nodes in the pool Worker-node hardware resources to be deployed 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;MachineDeployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;creationTimestamp\u0026#34;:null,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;clusterName\u0026#34;:\u0026#34;k8slorislombardi\u0026#34;,\u0026#34;replicas\u0026#34;:1,\u0026#34;selector\u0026#34;:{},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;spec\u0026#34;:{\u0026#34;bootstrap\u0026#34;:{\u0026#34;configRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;bootstrap.cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;KubeadmConfigTemplate\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;}},\u0026#34;clusterName\u0026#34;:\u0026#34;k8slorislombardi\u0026#34;,\u0026#34;infrastructureRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;infrastructure.cluster.x-k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VCDMachineTemplate\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;k8slorislombardi-worker-node-pool-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;k8slorislombardi-ns\u0026#34;},\u0026#34;version\u0026#34;:\u0026#34;v1.21.11+vmware.1\u0026#34;}}},\u0026#34;status\u0026#34;:{\u0026#34;availableReplicas\u0026#34;:0,\u0026#34;readyReplicas\u0026#34;:0,\u0026#34;replicas\u0026#34;:0,\u0026#34;unavailableReplicas\u0026#34;:0,\u0026#34;updatedReplicas\u0026#34;:0}} cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;5\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; capacity.cluster-autoscaler.kubernetes.io/memory: \u0026#34;4\u0026#34; capacity.cluster-autoscaler.kubernetes.io/cpu: \u0026#34;2\u0026#34; capacity.cluster-autoscaler.kubernetes.io/ephemeral-disk: \u0026#34;20Gi\u0026#34; capacity.cluster-autoscaler.kubernetes.io/maxPods: \u0026#34;200\u0026#34; machinedeployment.clusters.x-k8s.io/revision: \u0026#34;1\u0026#34; Deploying the autoscaler We use the following yaml to use the previously created PVC to retrieve the kubeconfig.\nBe sure to replace k8slorislombardi and k8slorislombardi-ns with your cluster parameters\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: k8slorislombardi-ns labels: app: cluster-autoscaler spec: selector: matchLabels: app: cluster-autoscaler replicas: 1 template: metadata: labels: app: cluster-autoscaler spec: containers: - image: us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.20.0 name: cluster-autoscaler command: - /cluster-autoscaler args: - --cloud-provider=clusterapi - --kubeconfig=/data/config - --cloud-config=/data/config - --node-group-auto-discovery=clusterapi:clusterName=k8slorislombardi - --namespace=k8slorislombardi-ns - --node-group-auto-discovery=clusterapi:namespace=default volumeMounts: - name: pvc-autoscaler mountPath: /data volumes: - name: pvc-autoscaler persistentVolumeClaim: claimName: pvc-autoscaler serviceAccountName: cluster-autoscaler terminationGracePeriodSeconds: 10 --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-workload roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-workload subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: k8slorislombardi-ns --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-management roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-management subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: k8slorislombardi-ns --- apiVersion: v1 kind: ServiceAccount metadata: name: cluster-autoscaler namespace: k8slorislombardi-ns --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-workload rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces - persistentvolumeclaims - persistentvolumes - pods - replicationcontrollers - services verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - list - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/eviction verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - storage.k8s.io resources: - csinodes - storageclasses - csidrivers - csistoragecapacities verbs: - get - list - watch - apiGroups: - batch resources: - jobs verbs: - list - watch - apiGroups: - apps resources: - daemonsets - replicasets - statefulsets verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - delete - get - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - get - update --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-autoscaler-management rules: - apiGroups: - cluster.x-k8s.io resources: - machinedeployments - machinedeployments/scale - machines - machinesets verbs: - get - list - update - watch Check\n1 2 3 4 5 6 7 8 9 10 11 k apply -f .\\autoscale.yaml deployment.apps/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler-workload created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler-management created serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-workload created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-management created k get pod NAME READY STATUS RESTARTS AGE cluster-autoscaler-79c5cb9df6-gqlfd 1/1 Running 0 29s rdeprojector configuration By default, Cloud director continuously checks the configuration of your Kubernetes cluster via the rdeprojector controller. This controller can also be used to add or remove workers from the cloud director GUI.\n1 k edit deployments.apps rdeprojector-controller-manager -n rdeprojector-system Scale down th replica at 0\n1 2 3 4 5 6 7 spec: progressDeadlineSeconds: 600 replicas: 0 revisionHistoryLimit: 10 selector: matchLabels: control-plane: controller-manager Test Autoscaling 1 2 3 4 k apply -f po-wget.yaml k exec -it pod-wget -- sh / # while true; do wget -q -O- http://172.31.7.210;done OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! As seen above, a few minutes later we can the a hight CPU utilization for the Pods and the Worker node.\n1 2 3 k get hpa -n default NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-example-autoscaler Deployment/hpa-example 122%/50% 1 10 8 3h17m The autoscaler goes into action and deploys a new Worker-node\n1 2 3 k get machinedeployments.cluster.x-k8s.io NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION k8slorislombardi-worker-node-pool-1 k8slorislombardi 2 1 2 1 ScalingUp 24h v1.21.11+vmware.1 Automatic integration of the new Worker-node\n1 2 3 4 5 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 24h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-fk2xn NotReady \u0026lt;none\u0026gt; 3s v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 24h v1.21.11+vmware.1 The new Worker is Ready\n1 2 3 4 5 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 24h v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-fk2xn Ready \u0026lt;none\u0026gt; 42s v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 24h v1.21.11+vmware.1 Additional pods deployed\n1 2 3 k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE hpa-example 10/10 10 10 3h29m Source : [cluster-autoscaler]https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\n","date":"2023-06-18T00:00:00Z","image":"https://www.loris-lombardi.cloud/p/cse-kubernetes-hpa-autoscaling/cse-auto-scaling_hu6f5b223d0965fa925062afe10f8ce322_369195_120x120_fill_box_smart1_3.png","permalink":"https://www.loris-lombardi.cloud/p/cse-kubernetes-hpa-autoscaling/","title":"Container Service Engine autoscaling"},{"content":"In this article we\u0026rsquo;re going to install a new storage driver for persistent data for your containers hosted on a Tanzu TKGm cluster provided by Cloud Director and Container Service Engine (CSE).\nWhy use another storage driver? Clusters deployed by CSE are installed by default with the following driver cloud-director-named-disk-csi-driver\nHowever, this driver has a number of limitations, particularly with regard to access modes\nCSI Cloud Director Feature matrix\nFeature Support Scope Storage Type Independent Shareable Named Disks of VCD Provisioning Static Provisioning, Dynamic Provisioning Access Modes ReadOnlyMany , ReadWriteOnce Volume Block VolumeMode FileSystem Topology Static Provisioning: reuses VCD topology capabilities, Dynamic Provisioning: places disk in the OVDC of the ClusterAdminUser based on the StorageProfile specified. As can be seen from the table above, taken from the default driver documentation, it is not possible to create ReadWriteMany PVCs. This can be problematic if several Pods wish to write data to the same PVC.\nNor is it possible to assign more than 15 volumes per worker node with this driver.\n1 2 3 4 5 6 7 8 9 10 11 // https://github.com/vmware/cloud-director-named-disk-csi-driver/blob/main/pkg/csi/node.go const ( // The maximum number of volumes that a node can have attached. // Since we\u0026#39;re using bus 1 only, it allows up-to 16 disks of which one (#7) // is pre-allocated for the HBA. Hence we have only 15 disks. maxVolumesPerNode = 15 DevDiskPath = \u0026#34;/dev/disk/by-path\u0026#34; ScsiHostPath = \u0026#34;/sys/class/scsi_host\u0026#34; HostNameRegexPattern = \u0026#34;^host[0-9]+\u0026#34; ) CSI NFS driver installation Requirement A functional Kubernetes cluster Some Kubernetes knowledge Helm NFS server and PVC In this example we have a Kubernetes cluster deployed by CSE 4.0.3, with one a master node and one worker node in version 1.21.\n1 2 3 4 k get node NAME STATUS ROLES AGE VERSION k8slorislombardi-control-plane-node-pool-njdvh Ready control-plane,master 6h19m v1.21.11+vmware.1 k8slorislombardi-worker-node-pool-1-69f68cc6b9-kfhz9 Ready \u0026lt;none\u0026gt; 6h16m v1.21.11+vmware.1 We install the driver with helm\n1 2 helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system We need to modify the default configuration of the Helm chart. You can edit the deployments and daemonsets configuration directly or modify the values.yaml file in the helm repository.\nModify dnsPolicy\n1 2 k edit deployments.apps csi-nfs-controller -n kube-system k edit daemonsets.apps csi-nfs-node -n kube-system Replace\n1 dnsPolicy: Default With\n1 dnsPolicy: ClusterFirstWithHostNet Check\n1 2 3 4 k get pod -n kube-system | grep csi-nfs csi-nfs-controller-64cc5764b7-bd4p5 3/3 Running 0 6m37s csi-nfs-node-fp45z 3/3 Running 0 6m37s csi-nfs-node-vwf44 3/3 Running 0 6m37s We\u0026rsquo;re going to create a PVC that will be used by our NFS server. This PVC is created via the default driver: cloud-director-named-disk-csi-driver.\nBe sure to configure the size of this PVC according to your needs and future requirements, as the size of this PVC cannot be modified.\nIn this example, the cloud-director-named-disk-csi-driver is provide by the StorageClass default-storage-class-1\n1 2 3 k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false Example of a PVC for NFS server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # pvc-nfs-server.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-provisioner: named-disk.csi.cloud-director.vmware.com name: pvc-nfs-csi-server spec: accessModes: - ReadWriteOnce resources: requests: storage: 1024Gi storageClassName: default-storage-class-1 # Use your StorageClasse Name volumeMode: Filesystem Next, we\u0026rsquo;ll create an NFS server in a dedicated namespace. This template will create an NFS server pod and also a ClusterIP service, which will be used by a new StorageClass for our future NFS PVCs.\nExemple d\u0026rsquo;un serveur NFS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 --- # nfs-server.yaml kind: Service apiVersion: v1 metadata: name: nfs-server labels: app: nfs-server spec: type: ClusterIP # use \u0026#34;LoadBalancer\u0026#34; to get a public ip selector: app: nfs-server ports: - name: tcp-2049 port: 2049 protocol: TCP - name: udp-111 port: 111 protocol: UDP --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-server spec: replicas: 1 selector: matchLabels: app: nfs-server template: metadata: name: nfs-server labels: app: nfs-server spec: nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux containers: - name: nfs-server image: itsthenetwork/nfs-server-alpine:latest env: - name: SHARED_DIRECTORY value: \u0026#34;/exports\u0026#34; volumeMounts: - mountPath: /exports name: pvc-nfs-csi-server securityContext: privileged: true ports: - name: tcp-2049 containerPort: 2049 protocol: TCP - name: udp-111 containerPort: 111 protocol: UDP volumes: - name: pvc-nfs-csi-server persistentVolumeClaim: claimName: pvc-nfs-csi-server Setup NFS Server\n1 2 3 4 k create namespace nfs-csi kubens nfs-csi k apply -f pvc-nfs-server.yaml k apply -f nfs-server.yaml VÃ©rification\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-csi-server Bound pvc-d61aa727-313d-4466-b923-6121a1ce93f7 1Ti RWO default-storage-class-1 3m17s k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nfs-server 1/1 1 1 76s k get pod NAME READY STATUS RESTARTS AGE nfs-server-56dfcc48c8-w759j 1/1 Running 0 2m24s k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nfs-server ClusterIP 100.64.116.170 \u0026lt;none\u0026gt; 2049/TCP,111/UDP 3m12s Since we have created our service in the nfs-csi namespace, it responds to fqdn nfs-server.nfs-csi.svc.cluster.local\nNFS StorageClass Finally, we\u0026rsquo;ll create a new StorageClass using the previously created NFS server. You can also use another NFS server present in your environment (VM NFS, NFS NetApp\u0026hellip;).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # StorageClass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi provisioner: nfs.csi.k8s.io parameters: server: nfs-server.nfs-csi.svc.cluster.local # FQDN or IP of NFS server share: / reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - nfsvers=4.1 allowVolumeExpansion: true 1 2 3 4 5 k apply -f StorageClass.yaml k get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE default-storage-class-1 (default) named-disk.csi.cloud-director.vmware.com Delete Immediate false 7h8m nfs-csi nfs.csi.k8s.io Delete Immediate true 18s NFS driver test Setup NFS PVC and Pod 1 2 3 4 5 6 7 8 9 10 11 12 #test-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi storageClassName: nfs-csi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- # nginx-nfs-example.yaml apiVersion: v1 kind: Pod metadata: name: nginx-nfs-example spec: containers: - image: nginx name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/www name: test-pvc volumes: - name: test-pvc persistentVolumeClaim: claimName: test-pvc 1 2 k apply -f test-pvc.yaml k apply -f nginx-nfs-example.yaml Global check 1 2 3 4 5 6 7 k get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test-pvc Bound pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 5Gi RWX nfs-csi 7m41s k get pod NAME READY STATUS RESTARTS AGE nginx-nfs-example 1/1 Running 0 16s NFS Server\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubens nfs-csi k get pod NAME READY STATUS RESTARTS AGE nfs-server-7cccc9cc84-r2l7l 1/1 Running 0 65m k exec -it nfs-server-7cccc9cc84-r2l7l -- sh / # ls Dockerfile bin etc home media opt root sbin sys var README.md dev exports lib mnt proc run srv usr / # cd exports /exports # ls lost+found pvc-6e8e670d-73d8-4110-b6ed-8c6442b0e2c3 /exports # Source : csi-driver-nfs\n","date":"2023-06-17T00:00:00Z","image":"https://www.loris-lombardi.cloud/p/csi-nfs-container-service-engine/tkgm-csi-nfs_hu5ef57efeb7aec44e7c981e798331684a_122775_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.loris-lombardi.cloud/p/csi-nfs-container-service-engine/","title":"Setup NFS driver on Kubernetes TKGm cluster provide by Container Service Engine"}]